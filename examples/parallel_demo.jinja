
        <h1>Parallel LLM Query Execution Demo</h1>
        
        <h2>Independent Queries</h2>
        These queries will run in parallel:
        
        {% set topic1 = llmquery(prompt="Generate a short recipe name", model="gpt-4o-mini", temperature=0.8) %}
        {% set topic2 = llmquery(prompt="Generate a name for a technology startup", model="gpt-4o-mini", temperature=0.8) %}
        {% set topic3 = llmquery(prompt="Generate a creative book title", model="gpt-4o-mini", temperature=0.8) %}
        {% set topic4 = llmquery(prompt="Generate a name for a pet robot", model="gpt-4o-mini", temperature=0.8) %}
        
        <ul>
            <li>Recipe: {{ topic1 }}</li>
            <li>Startup: {{ topic2 }}</li>
            <li>Book: {{ topic3 }}</li>
            <li>Robot: {{ topic4 }}</li>
        </ul>
        
        <h2>Dependent Queries</h2>
        These queries depend on previous results:
        
        {% set details1 = llmquery(prompt="Write 2 sentences about a recipe called: " + topic1, model="gpt-4o-mini") %}
        {% set details2 = llmquery(prompt="Write 2 sentences about a startup called: " + topic2, model="gpt-4o-mini") %}
        
        <ul>
            <li>About the recipe: {{ details1 }}</li>
            <li>About the startup: {{ details2 }}</li>
        </ul>
        